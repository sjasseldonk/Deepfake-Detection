# -*- coding: utf-8 -*-
"""preprocess.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DvnLQE5yN3ksV3IRVcl080tcrS2wwysD
"""

import glob, os, cv2, time
import numpy as np
from sklearn.utils import shuffle
from tensorflow.keras.utils import to_categorical

def load_filepaths():
  # Load hand-crafted features from Openface
  x_real_filenames = [os.path.basename(x[:-4]) for x in glob.glob('/content/drive/My Drive/Thesis Deepfakes 2021/data_sampled/real_hand/*.csv')]
  x_fake_filenames = [os.path.basename(x[:-4]) for x in glob.glob('/content/drive/My Drive/Thesis Deepfakes 2021/data_sampled/fake_hand/*.csv')]

  # Sort the filenames to make sure right ids are sampled: real -> 1, fake -> 0
  x_real_filenames = sorted(x_real_filenames)
  x_fake_filenames = sorted(x_fake_filenames)
  y_real = np.ones(len(x_real_filenames))
  y_fake = np.zeros(len(x_fake_filenames))

  # Split data into 3 datasets: first 41 -> train, 41:50 -> validation, 50:58 -> test
  x_train, y_train = x_real_filenames[:410] + x_fake_filenames[:410], np.hstack((y_real[:410], y_fake[:410]))
  x_val, y_val = x_real_filenames[410:500] + x_fake_filenames[410:500], np.hstack((y_real[410:500], y_fake[410:500]))
  x_test, y_test = x_real_filenames[500:] + x_fake_filenames[500:], np.hstack((y_real[500:], y_fake[500:]))

  # Shuffle datasets here
  x_train, y_train = shuffle(x_train, y_train, random_state=456)
  x_val, y_val = shuffle(x_val, y_val, random_state=456)
  x_test, y_test = shuffle(x_test, y_test, random_state=456)

  # One hot encode y labels
  y_train = to_categorical(y_train, 2)
  y_val = to_categorical(y_val, 2)
  y_test = to_categorical(y_test, 2)

  print('x_train = {} , y_train {}'.format(len(x_train), y_train.shape))
  print('x_val = {} , y_val {}'.format(len(x_val), y_val.shape))
  print('x_test = {} , y_test {}'.format(len(x_test), y_test.shape))

  return x_train, y_train, x_val, y_val, x_test, y_test

def generate_frame_names(equal_interval):
  frame_indices = list(range(0, 138, equal_interval))
  frame_names = []

  for i in frame_indices:
    if i <10:
      frame_names.append(f'frame_det_00_00000{i+1}.npy')

    if i >= 10 and i<100:
      frame_names.append(f'frame_det_00_0000{i+1}.npy')

    if i >= 100:
      frame_names.append(f'frame_det_00_000{i+1}.npy')

  return frame_names

def euclidean_distance(point1, point2):
  '''Function that computes the euclidean distance between two 3d points.'''
  return np.sqrt(np.sum((point1-point2)**2, axis=1))

def load_batch_hand(x_batch, y_batch, n_frames, equal_interval=0):
  ''' [Good version] 
  This function loads in all extracted csv files from OpenFace. 
    Input:
      x_batch: filenames of data batch to load in
      y_batch: labels corresponding to x_batch
      n_frames: number of frames per video 
  
    Return: 
      x_batch: numpy array containing all hand-crafted features with shape: (batchsize, features)
  '''
  if equal_interval == 0:
    partial_x = np.zeros((len(x_batch), n_frames, 84), dtype=np.float32)
  else:
    frame_indices = list(range(0, 138, equal_interval)) # Shortest video has 138 frames
    partial_x = np.zeros((len(x_batch), len(frame_indices), 84), dtype=np.float32)

  # Column indexes of hand-crafted features from openface (mouth openness and mouth stretch are added later)
  feature_colindex = [11,12,296,297,298,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695]
  root = '/content/drive/My Drive/Thesis Deepfakes 2021/data_sampled/'

  # Loop over all filenames in x_batch_files
  for i, (filename, y) in enumerate(zip(x_batch, y_batch)):
    if y[0] == 1:
      # Load data
      data = np.genfromtxt((root+'fake_hand/'+filename+'.csv'), delimiter=',', skip_header=True)

      # Subselect frames
      if equal_interval == 0:
        data = data[:n_frames, :]
      else:
        data = np.array([data[i] for i in frame_indices])

      try:
        # Define new facial points: mouth openness, mouth stretch, nose features (4). Optional: eye openness of all frames
        uppermouth = data[:, [486, 554, 622]]
        lowermouth = data[:, [492, 560, 628]]
        leftmouthcorner = data[:, [483, 551, 619]]
        rightmouthcorner = data[:, [489, 557, 625]]
        tip_nose = data[:, [465, 533, 601]]

        # Calculate distances
        mouthopenness = euclidean_distance(uppermouth, lowermouth)
        mouthstretch = euclidean_distance(leftmouthcorner, rightmouthcorner)
        nose_leftmouth = euclidean_distance(tip_nose, leftmouthcorner)
        nose_rightmouth = euclidean_distance(tip_nose, rightmouthcorner)
        nose_uppermouth = euclidean_distance(tip_nose, uppermouth)
        nose_lowermouth = euclidean_distance(tip_nose, lowermouth)

        # Only select partial number of frames
        data = data[:, feature_colindex]

        # Add new features to the dataframe
        amplitude = np.column_stack([data, mouthopenness, mouthstretch, nose_leftmouth, nose_rightmouth, nose_uppermouth, nose_lowermouth])

        # Add velocity and acceleration vectors: SHOULD I MAKE ZEROS FOR FIRST FRAMES OR NOT
        amplitude_shifted = np.roll(amplitude, 1, axis=0)
        amplitude_shifted[:1] = 0
        velocity = amplitude_shifted - amplitude
        velocity_shifted = np.roll(velocity, 1, axis=0)
        velocity_shifted[:1] = 0
        acceleration = velocity_shifted - velocity

        f_hand = np.concatenate([amplitude, velocity, acceleration], axis=1)
        partial_x[i] = f_hand

      except Exception as e:
        print('An exception has occurred for csv file: {} with error type: {}'.format(filename, e))

    if y[1] == 1:
      # Search for file in real folder
      data = np.genfromtxt((root+'real_hand/'+filename+'.csv'), delimiter=',', skip_header=True)

      # Subselect frames
      if equal_interval == 0:
        data = data[:n_frames, :]
      else:
        data = np.array([data[i] for i in frame_indices])

      # Then calculate new features
      try:
        # Define new facial points: mouth openness, mouth stretch, nose features (4). Optional: eye openness of all frames
        uppermouth = data[:, [486, 554, 622]]
        lowermouth = data[:, [492, 560, 628]]
        leftmouthcorner = data[:, [483, 551, 619]]
        rightmouthcorner = data[:, [489, 557, 625]]
        tip_nose = data[:, [465, 533, 601]]

        # Calculate distances
        mouthopenness = euclidean_distance(uppermouth, lowermouth)
        mouthstretch = euclidean_distance(leftmouthcorner, rightmouthcorner)
        nose_leftmouth = euclidean_distance(tip_nose, leftmouthcorner)
        nose_rightmouth = euclidean_distance(tip_nose, rightmouthcorner)
        nose_uppermouth = euclidean_distance(tip_nose, uppermouth)
        nose_lowermouth = euclidean_distance(tip_nose, lowermouth)

        # Only select partial number of frames
        data = data[:, feature_colindex]

        # Add new features to the dataframe
        amplitude = np.column_stack([data, mouthopenness, mouthstretch, nose_leftmouth, nose_rightmouth, nose_uppermouth, nose_lowermouth])

        # Add velocity and acceleration vectors: SHOULD I MAKE ZEROS FOR FIRST FRAMES OR NOT
        amplitude_shifted = np.roll(amplitude, 1, axis=0)
        amplitude_shifted[:1] = 0
        velocity = amplitude_shifted - amplitude
        velocity_shifted = np.roll(velocity, 1, axis=0)
        velocity_shifted[:1] = 0
        acceleration = velocity_shifted - velocity

        f_hand = np.concatenate([amplitude, velocity, acceleration], axis=1)
        #print(f'[F_HAND] In total, {f_hand.size - np.count_nonzero(f_hand)} out of {f_hand.size} elements are zero.')

        partial_x[i] = f_hand

      except Exception as e:
        print('An exception has occurred for csv file: {} with error type: {}'.format(filename, e))
  
  # Normalize data to [0,1] interval
  p_min = np.array(partial_x).min()
  p_max = np.array(partial_x).max()
  partial_x = (partial_x - p_min) / (p_max - p_min)
  
  return partial_x

def load_batch_aligned(x_batch, y_batch, n_frames, equal_interval=0):
  '''[Good version] 
  Input:
    * x_batch: filepaths of batch of data 
    * y_batch: labels of data batch
    * n_frames: number of consecutive frames
    * equal_interval: if 0, then first N number of frames are taken, else, equal interval frames is taken

  Output: 
    * partial_x: actual data loaded in based on number of frames
  '''
  # Keras required shape = (batch_size, frames, height, width, channel) and data type numpy
  if equal_interval == 0:
    partial_x = np.zeros((len(x_batch), n_frames, 150, 150, 3), dtype=np.float32)
  else:
    frame_indices = list(range(0, 138, equal_interval)) # Shortest video has 138 frames
    frame_names = generate_frame_names(equal_interval)
    partial_x = np.zeros((len(x_batch), len(frame_indices), 150, 150, 3), dtype=np.float32)

  root = '/content/drive/My Drive/Thesis Deepfakes 2021/data_sampled/'

  # Loop over all filepaths in x_batch and load real data
  for i, (filename, y) in enumerate(zip(x_batch, y_batch)):
    try:
      if y[0] == 1:
        path = (root + 'fake_aligned/' + filename + '_aligned')
        
        # Read first n frames, else equal interval
        if equal_interval == 0:

          # Search in fake folder and return frames based on n_frames
          for (d, dirs, files) in os.walk(path):
            frames = [f for f in files if f.endswith('.npy')][:n_frames]
            for j, frame in enumerate(frames):
              im = np.load(d + '/' + frame)
              partial_x[i][j] = im

        else:
          # Search in fake folder and return frames based on equal interval
          j = 0
          for frame in [os.path.basename(x) for x in glob.glob(path + '/*.npy')]:
            if frame in frame_names:
              im = np.load(path + '/' + frame)
              partial_x[i][j] = im
              #print(f'[ALIGN] In total, {im.size - np.count_nonzero(im)} out of {im.size} pixels in this image are zero. EQUAL INTERVAL')
              j += 1

      elif y[1] == 1:
        path = (root + 'real_aligned/' + filename + '_aligned')

        if equal_interval == 0:
          # Search in fake folder and return all frames of 1 videofile in the batch sorted
          for (d, dirs, files) in os.walk(path):
            frames = [f for f in files if f.endswith('.npy')][:n_frames]
            for j, frame in enumerate(frames):
              im = np.load(d + '/' + frame)
              partial_x[i][j] = im

        else:
          # Search in fake folder and return frames based on equal interval
          j = 0
          for frame in [os.path.basename(x) for x in glob.glob(path + '/*.npy')]:
            if frame in frame_names:
              im = np.load(path + '/' + frame)
              partial_x[i][j] = im
              j += 1

    except Exception as e:
      print('An exception has occurred for algined file: {} with error type: {}'.format(filename, e))

  # Normalize the whole batch at once
  partial_x /= 255.0

  return partial_x